---
title: "Introduction to R for Data Analysis"
author: "James Cutler"
date: "`r library(dplyr); Sys.Date() %>% format('%A, %d %B %Y')`"
# output: 
#   bookdown::pdf_document2:
#     fontsize: 12pt
#     number_sections: false
#     # fig_caption: true
#     toc: true
#     fig_height: 3
output:
  rmdformats::readthedown:
    use_bookdown: yes
    number_sections: no
    highlight: tango
    lightbox: yes
    gallery: yes
    df_print: paged
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = F,
                      message = F,
                      error = F)
```

# Some Guidelines

1.  Don't use base `R` code if you can help it. Use `tidyverse` instead. I've done both---so take my word for it!

2.  Don't use base `R`. Use RStudio!!! I will hunt you down if you ever open `R` instead of `RStudio`.

3.  **Get familiar with the hot-keys** (keyboard short cuts), and **USE AUTOCOMPLETE**.

    1.  Assignment `<-` is alt+- (i.e. alt+"dash"). DO NOT TYPE THIS OUT MANUALLY UNLESS YOU HATE YOURSELF.

    2.  The pipe `%>%` (see below) is shift+ctrl+m on windows or shift+command+m on a mac. Do not type this out manually.

    3.  Comment out as much code at a time as you want by highlighting it and hitting shift+ctrl+c on windows or shift+command+c on mac.

    4.  Run code by hitting ctrl+enter on windows or command+enter on mac. You don't have to highlight the code! I only highlight when I'm running lots of code at once.

    5.  In RMarkdown, alt+ctrl+i on windowns or alt+command+i on mac adds an `R` code chunk for you.

    6.  In RStudio, you can place the cursor in the console with ctrl+2 (windows and mac) and you can place it back in the editor with ctrl+1.

    7.  In RStudio, you can open up a new script (a new tab) in the editor pane with shift+ctrl+n on windows and shift+command+n on mac.

    8.  In RStudio, move between tabs (scripts) with ctrl+alt and a left or right arrow (ctrl+option on a mac).

4.  Don't get misled by the myths out there about SAS vs R (e.g., `R` stuff is "home-cooked" ðŸ˜‰ðŸ˜‰)! Here's a couple links debunking some of those myths (while perpetuating at least one other myth---more on this later): [An old blogpost from 2014, part 1](https://thomaswdinsmore.com/2014/12/01/sas-versus-r-part-1/ "Mostly accurate material") and [Part 2 of the blogpost](https://thomaswdinsmore.com/2014/12/15/sas-versus-r-part-two/ "Mostly accurate material").

    1.  At the end I'll provide more links to resources demonstrating the versatility, professional quality, and reliability of `R`.

5.  Write your code legibly. Try to hit enter, and align things vertically, as often as is reasonable. Don't write lines of code that run off the page to the right if you can help it.

6.  Annotate your code! Use the four dashes, the document outline, and multiple hashtags for indentation.

7.  What should you do if you get an error? Copy the error message, or part of the error message you think is most important, and paste it into a google search. I've done this hundreds of times! It usually gets answers in seconds.

Before we get started on any data analysis in `R`, we load the libraries we are going to need. 80% of everything I do I can do with the `tidyverse` and base `R` libraries. But beyond those two, I would say I use about a dozen or so others on a somewhat regular basis. I've probably only ever used little more than a few dozen or so libraries in my life. So while there are literally thousands of `R` libraries, you won't need to get familiar with more than a relative few!

```{r}
pacman::p_load(
  readxl,
  sas7bdat,
  tidyverse,
  tidyquant,
  tidymodels,
  scales,
  plotly,
  gt,
  gtsummary,
  
  usmap,
  timetk,
  leaflet,
  
  survival,
  survminer,
  nlme,
  # lme4 # Not used here. But great for mixed models!
)
```

I like to use custom functions (functions I write myself) in order to save myself time and hassle. Open the 00_custom_functions.R script to see what they are! Writing functions in `R` will be briefly introduced below.

```{r}
source("00_custom_functions.R")
```

# The Basics

Much of the material presented here, and a lot more, can be found---in a *very* helpful way---at [R for Data Science](https://r4ds.had.co.nz/ "R for Data Science, a free online book by Hadley Wickham"). This book is amazing! Ignore it at your own risk.

You can use `R` as a calculator:

```{r}
3 + 5^3 - sin(4) / (pi*log10(100))
```

You can assign things to "objects". For this, use alt+-, i.e., alt+dash:

```{r}
X <- rnorm(100)
Y <- X + rnorm(100)

my_tibble <- tibble(
  x = X,
  y = Y
)

my_tibble
```

If you want to generate, say, 100 random variates from the standard normal distribution, it's as easy as `rnorm(100)`. Lots of functionality for all the other common distributions are, of course, available in `R`.

Here is `X`:

```{r}
X
```

```{r}
length(X)
```

And here is `Y`:

```{r}
Y
```

You can plot `x` and `y`:

```{r}
my_tibble %>% 
  ggplot(aes(x,y)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_tq() +
  labs(
    title = "This is a scatter plot with a linear OLS fitted regression line",
    x = "X",
    y = "Y"
  )
```

You can write your own functions (and your own `R` libraries, if you want!):

```{r}
make_a_demo_data_tibble <- function(X){
  
  # Make y
  Y <- X + rnorm(length(X))
  
  my_tibble <- tibble(
    x = X,
    y = Y
  )
}

foo <- make_a_demo_data_tibble(X = rnorm(200))

foo
```

There are easy ways of making vectors or regular sequences of numbers:

```{r}
seq(-4,4,length.out = 100) -> a_sequence

a_sequence %>% as_tibble()
```

Do you recognize this formula?

```{r}
y <- ( 1/sqrt(2*pi) ) * exp(-.5*a_sequence^2) 

plot(a_sequence,y)
```

## The Pipe

What's this `%>%` thing? It's called the "pipe" operator. The hot-key for it is shift+ctrl+m on windows, or shift+command+m if you're on a mac. If you're familiar with bash, it's kind of like the pipe in bash code. It takes the output of the code before the pipe, and "pipes" it into the code after the pipe as the first argument in whatever function is sitting there. Not using the pipe means writing more redundant and disjointed code that doesn't flow nearly well enough to make you really productive with your time.

For example, this is what your code might look like without the pipe:

```{r}
Z <- X - 2*rnorm(100)
binary_variable <- rbinom(100,1,.5)

mutate(my_tibble,z = Z)       -> my_tibble
mutate(my_tibble,`x*z` = X*Z) -> my_tibble
mutate(
  my_tibble,
  b_var = binary_variable
)                             -> my_tibble

my_tibble
```

Notice all the redundant calls of `my_tibble` and the `mutate` function. But thanks to the pipe (and some other nice `tidyverse` features), we don't ever have to code like this again!! Let's see what this would be like *with* the pipe:

```{r}
# Start over with a fresh my_tibble
make_a_demo_data_tibble(rnorm(100)) -> my_tibble

my_tibble
```

```{r}
# Now use the pipe, and the fact that you can create multiple new variables/columns inside just one mutate function.
my_tibble %>% 
  rowwise() %>% 
  mutate(
    z     = x - 2*rnorm(1),
    `x*z` = x*z,
    b_var = rbinom(1,1,.5)
  )  %>% ungroup() -> my_tibble

my_tibble
```

That wasn't necessarily the best example of how the pipe will save you hours of time and megajoules of brain energy, but in order to demostrate a really good example, it would require a much more complex data wrangling situation. For those who are interested, I'd be happy to show many more use cases.

If by chance you don't see the intuitive nature of the pipe yet, find someone who can help you familiarize yourself with it through repeated practice, and very soon it will be as intuitive as putting one foot in front of the other! I'm happy to help anytime!

Also, if you're wondering what `rowwise` and `ungroup` are all about, don't sweat it. They're fairly self-explanatory. `rowwise` allows you to do data transformations or operations row-by-row. Since `rowwise` "groups" the data set into single-row groupings, you simply `ungroup` after you're done using `rowwise`. You'll get more familiar with it after you try it out a few times.

You may have noticed the `->` assignment arrow turned around in some of the code above. What's with this right-facing assignment arrow? Don't worry, it's just another cool illustration of the flexibility of `R`. You can point the assignment arrow either way, just as long as it's facing the object name, not the "stuff" you're assigning to the object.

There are a bajillion other things to mention about first things in R Basics before moving on to the next R Basics topic of data wrangling, but hopefully this gives you a glimpse of what basic syntax and objects in `R` are like.

# Data Wrangling & Visualization

It's been said, 80% of your quantitative analysis life is data wrangling. At least, it will be once you move outside of the classroom and into the real world.

This is just one more really big reason why you should use `R`, and the `tidyverse`.

## Overview of Data Types

There are numerics, integers, character strings, factors, dates, date-times, and other data types. For example:

```{r}
1:10
(1:10) %>% class()
```

```{r}
# Random uniform variates
runif(10)
runif(10) %>% class()
```

```{r}
letters[1:3]

letters[1:3] %>% class()
```

```{r}
letters[1:3] %>% factor()
letters[1:3] %>% factor() %>% class()
```

```{r}
timetk::tk_make_timeseries("2021-10-01","2021-10-10")
timetk::tk_make_timeseries("2021-10-01","2021-10-10") %>% class()
```

As with most of the stuff presented here, you can learn more [here](https://r4ds.had.co.nz/ "R for Data Science"). But these data types are fairly self-explanatory.

So let's dive in and wrangle some crappy data!

## BLS Employment Data

We'll import data already placed in our project environment, but you can also find this on the BLS website [here](https://www.bls.gov/oes/tables.htm "BLS Occupation and Employment Wage Statistics Data"), under May 2020; just click on XLS for "State" to download.

This is occupation wage data for the United States made available by the Bureau of Labor Statistics.

Notice that we have the help of the `janitor` package to "clean up" the names. We can also `select` which columns we want, `slice` the data to look at only the rows we want, and even tell `R` which excel sheet we want to import, as well as how many lines at the top of the spreadsheet to skip.

```{r}
oes_tbl <- readxl::read_excel("00_data/state_M2020_dl.xlsx",sheet = 1) %>% 
  janitor::clean_names()

oes_dict <- readxl::read_excel("00_data/state_M2020_dl.xlsx",sheet = 2,skip = 8) %>%
  janitor::clean_names() %>% select(1:2) %>% slice(1:31)
```

Let's say we're interested in the percentile wage columns, for hourly wage. We can look at the columns that start with "h\_" using the `select` and `matches` tidyverse functions:

```{r}
oes_tbl %>% select(matches("h_"))
```

Notice that the data isn't quite the format we want! It's in the character "chr" format. No problem. We just use the `mutate` function to convert it to numeric (what the `tidyverse` calls "double", or "dbl"), along with the `across` and `matches` functions, to make sure we mutate *across* all the columns whose names match the pattern "h\_". We can also select those columns to view them more easily, just to make sure we did it right, before storing the updated tibble:

```{r}
oes_tbl %>% mutate(across(matches("h_"),as.numeric)) %>% select(matches("h_"))
```

Note that NAs were introduced! That's okay. We should expect that when not everything in those columns was a number. It gives us a warning when that happens though just to make sure we're aware it happened.

(Don't get **warnings** and **errors** confused! Warnings are not errors. Warnings do *not* mean the code failed to execute. Warnings just let you know something particular occurred that you might want to know about.)

Now that we can see our changes worked, let's store that output (along with a selection of only the columns we want) as a new object---called `oes_2_tbl`.

```{r}
oes_tbl %>% 
  mutate(across(matches("h_"),as.numeric)) %>% 
  select(area_title,prim_state,occ_title,matches("h_")) %>% 
  rename(state_name = area_title) -> oes_2_tbl
```

Notice I also used `rename` to rename the column "area_title" to "state_name", just for the heck of showing you the `rename` function. We'll use another column-renaming function below.

Just curious, do we have all 50 states represented in our data? One visual way to confirm that is by plotting them on a map!

```{r}
usmap::plot_usmap(include = oes_2_tbl %>% 
                    distinct(prim_state) %>% 
                    slice(1:45) %>% 
                    pull(prim_state),
                  fill = "gray70") 
```

Notice that I "sliced" my data to only rows 1 through 45. I did that on purpose to show you a map with some states missing. We actually *do* have all the states though, as this map below confirms:

```{r}
usmap::plot_usmap(include = oes_2_tbl %>% 
                    distinct(prim_state) %>% 
                    slice(1:51) %>% 
                    pull(prim_state),
                  fill = "gray70") 
```

You would probably want a more sure-fire way of checking something like completeness in a list of states. What would you do in order to get an exact list of only the states you're missing?

(HINT: Try an anti-join using what you *know* to be a complete list of the states. An anti-join is something you can might want to look into later if you're not familiar with it yet.)

Now let's say I want to plot the wages at the various percentiles listed for a dozen or so different occupations. Some of the occupation names are extremely, ridiculously long. What's the *most efficient, most time-productive* way to do that?

Hmmm ...

One idea that occurred to me is to take advantage of `R`'s great auto-complete feature! (Not featured in SAS ðŸ˜¦). You can try this out yourself in the auxiliary script for this BLS data exercise---bls_data_practice.R. In summary, I would do something like this:

```{r}
oes_2_tbl %>% distinct(occ_title) %>% pull(occ_title) -> occupations

matrix(
  NA,
  nrow = 1,
  ncol = length(occupations)
) %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  set_names(occupations) -> occupations_tbl
```

Now I can simply type "occupations_tbl\$" and hit tab. All the occupation titles display in the auto-complete pop-up! I can simply start typing anything that pops into my head and see if it's there in this long list of 800+ different job titles!

You might be wondering what all that code means. I created an empty matrix with the specified dimensions (enough columns to match the number of occupation titles), and then converted that matrix to a tibble and set the column names to the occupation titles.

Now I've got my own list of jobs I'm interested in researching, by using copy and paste (came in handy for the really long names!):

```{r}
selected_occupations <- c(
  "Dentists, General",
  "Physicians, All Other; and Ophthalmologists, Except Pediatric",
  "Physician Assistants",
  "Dental Hygienists",
  "Nurse Practitioners",
  "Management Occupations",
  "Sales Managers",
  "Chief Executives",
  "Legislators",
  "Epidemiologists",
  "Statisticians",
  "Software Developers and Software Quality Assurance Analysts and Testers"
)
```

And now I'm ready to make a really cool interactive plot.

```{r fig.height=8}
oes_2_tbl %>% 
  filter(prim_state %in% c("OK","NY","TX","CA","UT","MA")) %>% 
  filter(occ_title %in% selected_occupations) %>% 
  filter(!(str_detect(occ_title,"Legisl"))) %>% 
  
  # Pivot longer
  pivot_longer(cols = h_mean:h_pct90) %>% 
  
  # Trim occupation title
  mutate(occ_title = str_sub(occ_title,end = 15)) %>% 
  
  # Plot
  ggplot(aes(name,value,color=occ_title,group=occ_title)) + 
  facet_wrap(~state_name) + 
  geom_point(size = 3) + 
  geom_line(size = 1.3) + 
  theme_tq() + scale_color_tq() + 
  theme(axis.text.x = element_text(angle = 30,hjust=.9,vjust=.9)) + 
  labs(title = "Hourly Wages by Occupation by State",
       x = "",y = "Dollars per Hour") -> g_wages

ggplotly(g_wages)
```

At this point you might be asking "What the heck is all that code supposed to mean?" Let's break it down:

1.  First I `filter`ed to just the states I was interested in. The `%in%` operator is similar to SAS's similarly-named operator.
2.  Then I did another `filter` to select just the occupations in my `selected_occupations` character vector created in the previous code chunk.
3.  I then highlighted and ran (using command+enter, or ctrl+enter on Windows) just the code I had written by the end of step 2, and noticed that there is no data at all for legislator! So in step 3 I removed the rows that detected a "Legisl" pattern (when I code sometimes I'm too lazy to type out the entire word).
4.  Then I pivoted to a longer tibble, because `R` (specifically, `ggplot2`) can much more productively use long rather than wide data. Notice that if you type the name of one column, a colon, and then the name of a column somewhere to the *right* of that column (doesn't matter how far away), `R` will select all the columns in between them, and including them, so you can save yourself the hassle of typing!
5.  Next I shortened the job titles to be 15 characters max.
6.  Then I plotted the data.

Here's the plot code breakdown:

1.  Inside `ggplot`, you use the aesthetics function called `aes` to plug in your x and y variables. Since `pivot_longer` defaults to calling the newly created key column "name" and the column with the corresponding values "value", I just plugged in name for x, and value for y. Note: You can highlight and run the code only up to (and including) the pivot_longer line, and see for yourself what those columns look like! Try it in the auxiliary script! (i.e., bls_data_practice.R)
2.  `facet_wrap` creates cool little facets---one for each state.
3.  `geom_point` plots the data as points, and `geom_line` plots lines connecting the points that you tell `R` are supposed to be connected (note that I did this with the group argument inside the aesthetics).
4.  `theme_tq` and `scale_color_tq` simply modify the theme and the colors of the plot to ones that I think look great.
5.  All that stuff in the `theme` function just angles the x axis text, so as to make it legible.
6.  `labs` adds your plot labels, of course.

I used the right-facing assignment arrow to dump the output of all that code into an object I called "g_wages". I use the letter "g" for "ggplot" plot objects.

Then I passed `g_wages` into the super awesome interactive plots function called `ggplotly` from the `plotly` library. And voila! Pretty cool. Try clicking on stuff once, and also twice, in the legend, and in the plot, to see what happens. You can look at any subset of jobs you like just by clicking on them! You can also hover over the plots to see exactly what the figures are for each job.

## Next Steps

There are so many other things that you can easily do, and easily learn to do, to wrangle all the crappy datasets you'll be handed in your career. I can't even begin to tell you how much easier your life will be if you use the `tidyverse` to wrangle and visualize data. See Martin Hadley's Lynda Learning course on the `tidyverse` for more, or see if I or someone else here can help with a particular question. Don't be afraid to ask! I'm happy to help.

# Biostatistics Core Methods

This section very very briefly exhibits the tip of the iceberg of things you can do in the following biostats methods:

-   multiple linear regression (biostats II)

-   analysis of frequency data (e.g., logistic regression)

-   survival analysis (e.g., Cox PH regression)

-   longitudinal data analysis

-   infectious disease methods (creating and plotting adjustable SEIR models with `R` `Shiny`)

-   multivariate analysis (e.g., machine learning methods, using `tidymodels`)

I know from personal experience that you can do everything taught in each of those BSE courses, plus other courses, in `R`.

There is a LOT more; this is just a brief intro.

Now let's dive into some research questions we can answer with these various multivariable models.

## Multiple Linear Regression

The burning question on everyone's mind: Is highway miles per gallon associated with manufacturer, number of cylinders, type of drive train, and does it improve with time (comparing the year 1999 to 2008)?

Call the `mpg` dataset (from the `tidyverse`).

```{r}
mpg
```

Pretty easy!

See [this tidyverse help page](https://ggplot2.tidyverse.org/reference/mpg.html "Fuel economy dataset description") for information about the meaning of the variable names.

Now store it as an object (not necessary, just something I do out of preference).

```{r}
mpg_tbl <- mpg
```

With minimal data preparation, we will be ready to fit a multivariable linear model.

Let's make Ford the reference group for the manufacturer variable (for no particular reason other than that it's a familiar manufacturer).

```{r}
mpg_tbl %>% 
  mutate(
    year = factor(year),
    manufacturer = factor(manufacturer),
    manufacturer = relevel(manufacturer,ref = "ford")
  ) -> mpg_tbl
```

Now fit a linear model!

We're just choosing a few of the variables available. We will not include interactions, for the sake of simplicity since this is just an intro to R.

```{r}
fit_lm <- lm(
  hwy ~ drv + cyl + year + manufacturer,
  data = mpg_tbl
)
```

The syntax is nice. It's kind of like writing a formula, inside the `lm()` function.

Now spit out a table for people to see the regression results.

```{r}
fit_lm %>% tbl_regression()
```

And spit out a dot-whisker plot for *MUCH* better visualization of the regression results.

```{r}
fit_lm %>% tidied_model_lm() %>% 
  plot_dotwhisker(
    my_title = "Highway Miles Per Gallon Adjusted Linear Relationships"
  )
```

See why I tried to limit the number of variables in the model? Categorical variables are actually not just one variable!

There is a great deal of flexibility and versatility with both the model prep, the model fitting, and the communication of results---this is just the tip of the iceberg! Just one example of what you can do.

For example, when it comes to communicating or displaying model results, you can opt for no labels on the dot-whisker plot:

```{r}
fit_lm %>% tidied_model_lm() %>% 
  plot_dotwhisker(
    my_title = "Highway Miles Per Gallon Adjusted Linear Relationships",
    labels = F
  )
```

## Multiple Logistic Regression

Are nodule presence and nature, as well as prostate specific antigen (PSA) concentration, associated with capsule penetration of a prostate tumor?

A logistic regression model is virtually the same, syntax-wise, as our linear model above, except we use `glm()` instead of `lm()`.

First, prepare the data to be properly analyzed. We'll want to make capsule (the variable for capsule penetration yes/no, i.e., 1/0) a factor with the reference group set to the 0 (no penetration).

We'll leave nodule nature as a numeric (i.e., a continuous/ordinal variable, with possibilities 1, 2, 3, and 4, representing no nodule, unilobar left, unilobar right, and bilobar, respectively).

```{r}
prostate_tbl <- read_csv("00_data/Prostate_Frequency_Class.csv")

prostate_tbl %>% 
  mutate(
    capsule = factor(capsule),
    capsule = relevel(capsule,ref = "0")
  ) -> prostate_tbl

prostate_tbl
```

Now fit the model. The only new thing is that you specify a link function---in this case the logit link, which is the default when you set the `family` argument equal to binomial.

```{r}
fit_glm <- glm(
  capsule ~ dpros + psa,
  data   = prostate_tbl,
  family = binomial
)
```

Now spit out the results.

First the table of the results:

```{r}
fit_glm %>% tbl_regression(exponentiate = T)
```

Then a dot-whisker plot:

```{r}
fit_glm %>% tidied_model_glm() %>% 
  plot_dotwhisker(
    my_title = "Adjusted Odds Ratios for the Model Covariates",
    x_axis   = "Odds Ratios (and 95% CIs)",
    x_intercept = 1
  )
```

Cool beans.

## Survival Analysis (Cox PH Regression)

Question: Is overall survival for ovarian cancer patients associated with age, cancer stage (as a continuous variable), and cancer antigen 125?

The `survival` package is great for survival analysis. Other supporting packages include, primarily, `survminer`, such as for plotting data using the function `ggsurvplot`.

Load the data:

```{r}
ovarian_tbl <- readxl::read_excel("00_data/17. CA125 in Ovary Cancer.xlsx") %>% 
  janitor::clean_names()

ovarian_tbl
```

Prep the data:

```{r}
ovarian_tbl %>% 
  mutate(
    # Prep age_group
    age_group = recode(age_group,
                       `1` = "Up to 55 yo",
                       `2` = "Over 55 yo"),
    age_group = factor(age_group),
    age_group = relevel(age_group,ref = "Up to 55 yo"),
    
    # Prep ca125_level
    ca125_level = recode(ca125_level,
                         `1` = "Normal",
                         `2` = "High"),
    ca125_level = factor(ca125_level),
    ca125_level = relevel(ca125_level,ref = "Normal")
  ) -> ovarian_tbl
```

Fit the model:

```{r}
fit_coxph <- coxph(
  Surv(os,status) ~ age_group + ca125_level + stage,
  data = ovarian_tbl
)
```

Share the results:

```{r}
fit_coxph %>% tbl_regression(exponentiate = T)
```

```{r}
fit_coxph %>% tidied_model_glm() %>% 
  plot_dotwhisker(
    my_title = "Adjusted Hazard Ratios for the Model Covariates",
    x_axis = "Hazard Ratios (and 95% CIs)",
    x_intercept = 1
  )
```

## Longitudinal Data Analysis

Question: Is it lots of fun to fit a one-knot spline model, or just a little bit fun?

Load some longitudinal data about smoking and FEV, courtesy of our *Applied Longitudinal Analysis* textbook website.

```{r}
smoking_tbl <- sas7bdat::read.sas7bdat("00_data/smoking.sas7bdat") %>% 
    as_tibble() %>% 
    janitor::clean_names()
```

Some data transformation (don't worry about all this code, hopefully with some time spent trying it out step by step on your own, it will make sense!):

```{r}
smoking_tbl %>% 
    pivot_wider(names_from = time,values_from = fev1) %>% 
    mutate(baseline = `0`) %>% 
    pivot_longer(cols = `0`:`12`) %>% 
    rename(time = name,fev1 = value) %>% 
    mutate(
        change    = fev1 - baseline,
        time_rank = recode(time,
                           `0` = 1,
                           `3` = 2,
                           `6` = 3,
                           `9` = 4,
                           `12`= 5,
                           `15`= 6,
                           `19`= 7),
        time.f = factor(time,c(0,3,6,9,12,15,19)),
        time   = as.numeric(time),
        smoker = factor(smoker,c(0,1)), # The levels of smoker are 0 and 1
        id     = factor(id)
    ) %>% 
    arrange(id,time_rank) -> smoking_new_tbl
```

The knot:

```{r}
smoking_new_tbl %>% 
    rowwise() %>% 
    mutate(
        # The knot:
        years_post_6 = (time - 6)*I(time >= 6) %>% as.numeric()
    ) %>% 
    ungroup() -> smoking_new_tbl
```

The model:

```{r}
fit_F1_true <- gls(
  
    # The model formula
    fev1 ~  time + years_post_6 + smoker + smoker:time + smoker:years_post_6,
    
    corr    = corSymm(form = ~time_rank|id),    # Unstructured within-subject correlation 
    weights = varIdent(form = ~1|time_rank),    # Homogeneous variance across time
    data    = smoking_new_tbl %>% drop_na(fev1) # Drop FEV NAs or your results will be wrong
)
```

By the way, you can write $\LaTeX$ directly in RMarkdown! This is great when someone wants you, or you want, to write the model equation of a model you fit. Just enclose in single dollar signs for in-line LaTeX, and double dollar signs (not visible here, but you will see them in the .Rmd file) for LaTeX "chunks":

$$
Y_{ij} = \beta_1 + \beta_2Time_{ij} + \beta_3(Time_{ij} - 6)_+ + \beta_4Group_i + \beta_5Time_{ij}*Group_i + \beta_6(Time_{ij} - 6)_+*Group_i + e_{ij}
$$ Here's the one-knot spline model with estimated coefficients:

$$
\hat{Y}_{ij} = 3.6 - 0.04Time_{ij} + 0.01(Time_{ij} - 6)_+ - 0.3Group_i + 0.01Time_{ij}*Group_i - 0.03(Time_{ij} - 6)_+*Group_i
$$

Pretty cool ðŸ˜Ž.

## Infectious Disease Methods

To check out my HIV model (and a generic SEIR) model, visit my Shiny app [here](https://jcutler79.shinyapps.io/infectious_disease_model/ "Shiny is pretty darn cool.").

`Shiny` is the the way to easily make web apps just by writing `R` code! It opens up a whole new world for communicating your results to stakeholders. It allows them to interact with machine learning-based predictive models you would like to deploy, for example. You can get some practice publishing full stack web apps using shinyapps.io and the Lynda Learning tutorial on making interactive presenations linked below.

## Multivariate Analysis - Machine Learning

I can't begin to tell you how fun and exciting machine learning is in `R` with `tidymodels`. So I won't try in this brief presentation. Instead, I'll simply refer you to Julia Silge's AMAZING YouTube channel on `tidymodels` and machine learning, linked [here](https://www.youtube.com/c/JuliaSilge/videos "Julia Silge YouTube channel").

For now, suffice it to say that if you are not doing machine learning using `tidymodels`, you are *NOT* doing machine learning the right way unless you're living ten years ago!

Machine learning is a BIG topic, with lots of methods, and lots of great tools in the `tidymodels` ecosystem.

If you want to learn more feel free to reach out to me I'd be happy to help with what I can.

FYI, you can do everything in the BSE multivariate analysis course in R, including the few things in that class that they have you do in SAS. And everything you learn to do in that course in R, you can do more easily, painlessly and richly if you use `tidyverse` and `tidymodels` instead of base R.

(Just so you know!)

# Miscellaneous Things You Can Do 

## Maps

You can make interactive maps in `R` with `leaflet` (like JavaScript's Leaflet maps).

In this map below, we have a list of Oklahoma County high schools scraped from Wikipedia. They have already been geocoded in `R`.

```{r}
school_coords_tbl <- read_rds("00_data/OK_county_high_schools_coords.rds")

school_coords_tbl %>% my_jitter_coords(sd=.001) -> school_jitter_tbl
```

```{r}
school_jitter_tbl %>% 
  leaflet() %>% 
  addProviderTiles("CartoDB.Voyager") %>% 
  setView(lng = -97.5,lat = 35.5, zoom = 10) %>% 
  addCircleMarkers(
    label = ~labels,
    lat = ~lat,
    lng = ~lng,
    radius = 7
  )
```

The more-in-depth code behind this is in maps_and_web_scraping.R, which is in this GitHub repo.

To learn more about leaflet maps in `R` visit [RStudio's tutorials on the subject](http://rstudio.github.io/leaflet/ "Leaflet for R").

## Dashboards

See flexdashboard_and_dygraphs.Rmd (and the corresponding html file).

See also the link below to the gallery of dashboards you can make quickly and painlessly with flexdashboard.

# Miscellaneous Resources

Of course, there are SO many things we did not cover here. `R` is like a never-ending playground; only your research interests are the limit.

Here are a few random resources for more cool things to explore:

-   A gallery of extensions to `ggplot2` for all kinds of data visualization, [here](https://exts.ggplot2.tidyverse.org/gallery/ "A MUST-see!"). THIS GALLERY IS A MUST-SEE.

-   A `flexdashboard` gallery [here](https://rstudio.github.io/flexdashboard/articles/examples.html "EASY DASHBOARDS!"). ALSO A MUST-SEE.

-   Don't miss ... [The Big Book of R](https://www.bigbookofr.com/ "The Big Book of R"). It has every single online book about doing cool stuff in `R`!!

    -   Other links to lists of books on doing all kinds of research in R include [this one](https://www.routledge.com/Chapman--HallCRC-The-R-Series/book-series/CRCTHERSER?gclid=Cj0KCQjww4OMBhCUARIsAILndv76N6rAK-ouwdu6jVyDhWcXFaXeV9gMJy2WcNMG08yxuIcn_lCxelQaAkBbEALw_wcB "Chapman Hall/CRC The R Series"), and [this one](https://www.springer.com/series/6991?detailsPage=titles "Springer Nature Use R! Series").

-   Martin Hadley's "Learning the R Tidyverse" on Lynda Learning. Also see [R for Data Science](https://rpubs.com/ "R for Data Science"), by Hadley Wickham, linked above.

-   Side-by-side sample SAS code and R code for pretty much all the longitudinal data analysis exercises, provided by the authors of our BSE course's textbook, [here](https://content.sph.harvard.edu/fitzmaur/ala2e/ "Applied Longitudinal Analysis").

-   And don't forget to check out stackoverflow.com where you will find answers to literally 90% of your questions about how to do anything in `R` (as well as most other languages).

-   You can publish html documents made in `RMarkdown` directly to the web for free using [rpubs.com](https://rpubs.com/ "Easy Web Publishing from RStudio").

    -   You can learn how to do this in minutes by checking out Martin Hadley's Lynda Learning course called "Creating Interactive Presentations with Shiny and R", chapter 3 (about RPubs).

-   A wonderful list of how to do interesting things in `R` and cutting edge new data-wrangling and visualization tools by [Matt Dancho](https://github.com/business-science/free_r_tips "Free R Tips Weekly"). A MUST-SEE. Make sure you watch [the video](https://www.youtube.com/watch?v=F7aYV0RPyD0&list=PLo32uKohmrXtCExTRr-e8SxyQK6yT_Xri&ab_channel=BusinessScience "Set up a project on RStudio of someone else's github repo") on how to git pull the code in case you forget how!

-   If you want to browse through `R` packages by topic, visit [CRAN Task Views](https://cran.r-project.org/web/views/ "Lots of topics--and WAY more packages--here!").

-   I will be slowly, eventually adding to GitHub repositories all my code for each of the biostats methods courses I've taken (e.g., Analysis of Frequency Data, Survival Analysis, Multivariate Analysis, and Longitudinal Analysis).

-   Lastly, check out my thesis! That way for sure more than two people will have read it. It was all completely 100% done and written and knit to PDF in `R`. Check it out [here](https://www.proquest.com/docview/2446414689?pq-origsite=gscholar&fromopenview=true "James Cutler Master's Thesis").
